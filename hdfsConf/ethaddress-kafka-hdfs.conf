# 定义source，channel和sink组件
a5.sources = r1
a5.channels = c1
a5.sinks = k1

# 配置source参数，使用KafkaSource
a5.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a5.sources.r1.batchSize = 5000
a5.sources.r1.batchDurationMillis = 2000
a5.sources.r1.kafka.bootstrap.servers = skydp-server-4:9092,skydp-server-5:9092,skydp-server-6:9092
a5.sources.r1.kafka.topics = eth_address_data
a5.sources.r1.kafka.consumer.group.id = eth_group

# 使用memory channel传输数据
a5.channels.c1.type = file
a5.channels.c1.checkpointDir = /opt/script/flume/checkpoint/eth_address_data
a5.channels.c1.dataDirs = /opt/script/flume/data/eth_address_data

# 配置sink参数，将数据导入HDFS
a5.sinks.k1.type = hdfs
a5.sinks.k1.hdfs.path = /user/upload/eth_json/eth_address_data/%Y-%m-%d
a5.sinks.k1.hdfs.filePrefix = eth_address_data
a5.sinks.k1.hdfs.round = false

#a5.sinks.k1.hdfs.callTimeout=1900000

# 文件回滚参数
a5.sinks.k1.hdfs.rollInterval = 1800
a5.sinks.k1.hdfs.rollSize = 0
a5.sinks.k1.hdfs.rollCount = 0

# 使用lzo压缩
# a5.sinks.k1.hdfs.fileType = CompressedStream
# a5.sinks.k1.hdfs.codeC = lzop

# 连接各组件
a5.sources.r1.channels = c1
a5.sinks.k1.channel= c1
